{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, row_number, abs, count\n",
    "from pyspark.sql import Window, types\n",
    "\n",
    "spark_session = SparkSession.builder.enableHiveSupport().master(\"local\").getOrCreate()\n",
    "\n",
    "graph_path = \"/data/sample264\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "        \n",
    "    tops_df = df.withColumn('row_number', row_number().over(window))\\\n",
    "        .filter(col('row_number') <= n)\\\n",
    "        .drop(col('row_number')) \n",
    "        \n",
    "    tmp_df = tops_df.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias('sum_' + field))\n",
    "   \n",
    "    normalized_df = tops_df.join(tmp_df, key1, 'inner')\\\n",
    "        .withColumn('norm_' + field, col(field) / col('sum_' + field))\\\n",
    "        .cache()\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\t965774\n",
      "116\t867268\n",
      "128\t852564\n",
      "131\t880170\n",
      "195\t946408\n",
      "215\t860111\n",
      "235\t897176\n",
      "300\t857973\n",
      "321\t915545\n",
      "328\t943482\n",
      "333\t818202\n",
      "346\t864911\n",
      "356\t961308\n",
      "428\t943572\n",
      "431\t902497\n",
      "445\t831381\n",
      "488\t841340\n",
      "542\t815388\n",
      "617\t946395\n",
      "649\t901672\n",
      "658\t937522\n",
      "662\t881433\n",
      "698\t935934\n",
      "708\t952432\n",
      "746\t879259\n",
      "747\t879259\n",
      "776\t946408\n",
      "784\t806468\n",
      "806\t866581\n",
      "811\t948017\n",
      "837\t799685\n",
      "901\t871513\n",
      "923\t879322\n",
      "934\t940714\n",
      "957\t945183\n",
      "989\t878364\n",
      "999\t967768\n",
      "1006\t962774\n",
      "1049\t849484\n",
      "1057\t920458\n"
     ]
    }
   ],
   "source": [
    "graph = spark_session.read.parquet(graph_path)\n",
    "\n",
    "unnorm = graph\\\n",
    "    .groupBy(col('userId'), col('trackId'))\\\n",
    "    .count()\n",
    "    \n",
    "norm = norm(unnorm, 'userId', 'trackId', 'count', 1000)\\\n",
    "    .select(col('userId'), col('trackId'))\\\n",
    "    .orderBy(col('norm_count').desc(), col('userId'), col('trackId'))\\\n",
    "    .limit(40)\n",
    "    \n",
    "result = norm.collect()\n",
    "\n",
    "for user_id, track_id in result:\n",
    "    print('%s\\t%s' % (user_id, track_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
